---
title: 'Two-stream graph convolutional networks with task-specific loss for dual-task gait analysis'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - Jiaqing Liu
  - Shuqiong Wu
  - admin
  - Yasushi Makihara
  - Yasushi Yagi

# Author notes (optional)
# author_notes:
#   - 'Equal contribution'
#   - 'Equal contribution'

date: '2023-07-24T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2017-01-01T00:00:00Z'

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ['paper-conference']

# Publication name and optional abbreviated publication name.
publication: In *Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2023)*
publication_short: In *EMBC 2023*

abstract: "Dual-task gait systems can be utilized to assess elderly patients for cognitive decline. Although numerous research studies have been conducted to estimate cognitive scores, this field still faces two significant challenges. Firstly, it is crucial to fully utilize dual-task cost representations for diagnosis. Secondly, the design of optimal strategies for effectively extracting dual-task cost representations remains a challenge. To address these issues, in this paper, we propose a deep learning-based framework that implements a spatio-temporal graph convolutional neural network (ST-GCN) with single-task and dual-task pathways for cognitive impairment detection in gait. We also introduce a novel loss, termed task-specific loss, to ensure that single-task and dual-task representations are distinguishable from each other. Furthermore, dual-task cost representations are calculated as the difference between dual-task and single-task representations, which are resilient to individual differences and contribute to the robustness of the framework. These representations provide a comprehensive view of single-task and dual-task gait information to generate task predictions. The proposed framework outperforms existing approaches with a sensitivity of 0.969 and a specificity of 0.940 for cognitive impairment detection."


# Summary. An optional shortened abstract.
#summary: We develop TreeFormer, accurately estimating plant skeletal structure from a single image.

tags:
  - EMBC 2023
  - EMBC
  - Biomedical
  - Dual tak

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://ieeexplore.ieee.org/document/10339953'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
# slides: example
---

<!-- {{% callout note %}}
Click the _Cite_ button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the _Slides_ button to check out the example.
{{% /callout %}}

Add the publication's **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). -->
